
import logging
from logging.config import fileConfig
import re
import os

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)
logger = logging.getLogger("alembic.env")


### Service Changes. We pull info from the main code. Like sites and tenants so we can add that data to the alembic.ini file.
from tapisservice.config import conf
from sqlmodel import create_engine, Session, select, SQLModel
from stores import pg_store, pg_default

# Get config settings to create conninfo later
username = conf.postgres_user
password = conf.postgres_pass
host = conf.postgres_host


all_urls = {}
# Create databases and schemas wanted.
for site, tenants in pg_store.items():
    # Create database and fail gracefully if it already exists
    try:
        pg_default.run(f'CREATE DATABASE "{site}"', autocommit=True)
    except:
        msg = f"Database for site: {site}, already exists. Skipping."
        logger.warning(msg)
    
    # Create schemas for each tenant
    for tenant, pg in tenants.items():
        try:
            # TODO indexes! #CREATE CONSTRAINT FOR (p:Pod) REQUIRE p.name IS UNIQUE
            pg.run(f'CREATE SCHEMA IF NOT EXISTS "{tenant}"', autocommit=True)
        except Exception as e:
            msg = f"Error when creating schemas for tenant: {tenant}. e: {repr(e)}"
            logger.warning(msg)

        # Add database connection info to alembic config
        conninfo = f"postgresql://{username}:{password}@{host}/{site}"#?options=-csearch_path%3Ddbo,{tenant}"
        name = f"{site}_{tenant}".replace("-", "HYPHEN") # Some tenants have - in their names. This messes up alembic later.
        all_urls[name] = conninfo
        config.set_section_option(name, "sqlalchemy.url", conninfo)

# Add databases to alembic's list of databases.
config.set_main_option("databases", ",".join(all_urls.keys()))

# gather section names referring to databases.
db_names = config.get_main_option("databases")
logger.warning(f"Using the following databases with alembic: {db_names}")

######### Import all of the models we want to be autogenerated. Will proliferate to all schemas.
from models import ExportedData, Pod, Password
target_metadata = SQLModel.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_example_option = config.get_main_option("my_example_option")


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    # for the --sql use case, run migrations for each URL into
    # individual files.

    engines = {}
    for name in re.split(r",\s*", db_names):
        engines[name] = rec = {}
        rec["url"] = context.config.get_section_option(name, "sqlalchemy.url")

    for name, rec in engines.items():
        logger.info("Migrating database %s" % name)
        file_ = "%s.sql" % name
        logger.info("Writing output to %s" % file_)
        with open(file_, "w") as buffer:
            context.configure(
                url=rec["url"],
                output_buffer=buffer,
                target_metadata=target_metadata,
                literal_binds=True,
            )
            with context.begin_transaction():
                context.run_migrations(engine_name=name)


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    # for the direct-to-DB use case, start a transaction on all
    # engines, then run all migrations, then commit all transactions.
    engines = {}
    for name in re.split(r",\s*", db_names):
        engines[name] = engine_from_config(
            context.config.get_section(name),
            prefix="sqlalchemy.",
            poolclass=pool.NullPool,
        )

    # Create transactions/connections here so we can rollback/close later.
    connections = []
    transactions = []
    try:
        for name, engine in engines.items():
            site, tenant = name.split('_')
            tenant = tenant.replace('HYPHEN', '-')

            ## GO THROUGH ALL TENANTS, MEANING MIGRATE TO EACH SCHEMA!
            logger.info(f"Migrating database {site}; tenant {tenant}")
            # Reference https://alembic.sqlalchemy.org/en/latest/cookbook.html#rudimental-schema-level-multi-tenancy-for-postgresql-databases
            # Get engine/connection/transaction and add some schema stuff to it.
            connection = engine.connect()
            connection.execute(f'set search_path to "{tenant}"')
            connection.dialect.default_schema_name = tenant
            connections.append(connection)
            # Create transaction
            transaction = connection.begin()
            transactions.append(transaction)
            # Create context
            context.configure(
                connection=connection,
                upgrade_token=f"{name}_upgrades",
                downgrade_token=f"{name}_downgrades",
                target_metadata=target_metadata,
            )
            context.run_migrations(engine_name=name)

        for transaction in transactions:
            transaction.commit()
    except:
        for transaction in transactions:
            transaction.rollback()
        raise
    finally:
        for connection in connections:
            connection.close()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
